{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "864eea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library and create spark session\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10f8c68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/26 05:15:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# create the spark session\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"test\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebc02443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet('data/pq/green/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b9bde88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'lpep_pickup_datetime',\n",
       " 'lpep_dropoff_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'ehail_fee',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'payment_type',\n",
       " 'trip_type',\n",
       " 'congestion_surcharge']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143a757f",
   "metadata": {},
   "source": [
    "To compare with the Spark SQL we used in previous lecture as below to show the revenue of greeen taxi, we can do it in RDD as wel by implementing its own strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60657010",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "SELECT\n",
    "   \n",
    "    date_trunc('hour', pickup_datetime) AS hour,\n",
    "    PULocationID AS zone,\n",
    "    \n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "    \n",
    "FROM\n",
    "    green\n",
    "WHERE\n",
    "    pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1,2\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ed732",
   "metadata": {},
   "source": [
    "Within the dataframe, we have the RDD as the architeture inside. For example, we can use the df_green to access the RDD as follow. The `take` command is used to access the rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d72fa67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 23, 13, 10, 15), lpep_dropoff_datetime=datetime.datetime(2020, 1, 23, 13, 38, 16), store_and_fwd_flag='N', RatecodeID=1, PULocationID=74, DOLocationID=130, passenger_count=1, trip_distance=12.77, fare_amount=36.0, extra=0.0, mta_tax=0.5, tip_amount=2.05, tolls_amount=6.12, ehail_fee=None, improvement_surcharge=0.3, total_amount=44.97, payment_type=1, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=None, lpep_pickup_datetime=datetime.datetime(2020, 1, 20, 15, 9), lpep_dropoff_datetime=datetime.datetime(2020, 1, 20, 15, 46), store_and_fwd_flag=None, RatecodeID=None, PULocationID=67, DOLocationID=39, passenger_count=None, trip_distance=8.0, fare_amount=29.9, extra=2.75, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=33.45, payment_type=None, trip_type=None, congestion_surcharge=None)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the rdd \n",
    "df_green.rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89bf6d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create something similar to the query, we first extract the columns that will be used \n",
    "rdd = df_green.select('lpep_pickup_datetime', 'PULocationID','total_amount').rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156cbd98",
   "metadata": {},
   "source": [
    "To achieve the \"WHERE pickup_datetime >= '2020-01-01 00:00:00'\" clause in the SQL query, we need to use the filter the filter take in function to transform the data. In such case, we will be creating a customize fucntion \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9bba250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_date = datetime(year=2020, month=1,day=1)\n",
    "\n",
    "def filter_outliers(row):\n",
    "    return row['lpep_pickup_datetime']>=start_date\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "006e81f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-23 13:10:15\n",
      "2020-01-20 15:09:00\n"
     ]
    }
   ],
   "source": [
    "# to access the individual row\n",
    "for row in rdd.take(2):\n",
    "    print(row['lpep_pickup_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a75fb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 23, 13, 10, 15), PULocationID=74, total_amount=44.97),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 20, 15, 9), PULocationID=67, total_amount=33.45),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 15, 20, 23, 41), PULocationID=260, total_amount=8.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 5, 16, 32, 26), PULocationID=82, total_amount=8.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 29, 19, 22, 42), PULocationID=166, total_amount=12.74)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(filter_outliers).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52637923",
   "metadata": {},
   "source": [
    "Next, to achieve the Group By in the SQL query, it can be achieve by map() function in RDD which take in rows as input and output as rows as well. It prepare the 'key' and 'value' used for reshuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b764532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_grouping(row):\n",
    "    hour = row.lpep_pickup_datetime.replace(minute=0,second=0,microsecond=0)\n",
    "    zone = row.PULocationID\n",
    "    key = (hour, zone)\n",
    "    \n",
    "    amount = row.total_amount\n",
    "    count = 1\n",
    "    value = (amount, count)\n",
    "\n",
    "    \n",
    "    return (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6dcd0e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((datetime.datetime(2020, 1, 23, 13, 0), 74), (44.97, 1)),\n",
       " ((datetime.datetime(2020, 1, 20, 15, 0), 67), (33.45, 1)),\n",
       " ((datetime.datetime(2020, 1, 15, 20, 0), 260), (8.3, 1)),\n",
       " ((datetime.datetime(2020, 1, 5, 16, 0), 82), (8.3, 1)),\n",
       " ((datetime.datetime(2020, 1, 29, 19, 0), 166), (12.74, 1))]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = rdd.filter(filter_outliers)\\\n",
    "    .map(prepare_for_grouping)\\\n",
    "    .take(5)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc0370f",
   "metadata": {},
   "source": [
    "Using the reduce_by_key() function, we then can group them accordingly to the key, which the reshuffling occur. Similar to map(), it take in RDD and output RDD as well but replication of key is eliminated by reducing them into one. The reduce_by_key() function will automatically sort the similar key together. Hence, the reduce action will be focusing on the value aggregration of two rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b157f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(left_value,right_value): # calculate revenue\n",
    "    left_amount, left_count = left_value\n",
    "    right_amount, right_count = right_value\n",
    "    \n",
    "    output_amount = left_amount + right_amount\n",
    "    output_count = left_count + right_count\n",
    "    \n",
    "    return (output_amount, output_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39b858b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((datetime.datetime(2020, 1, 15, 20, 0), 260), (163.90000000000003, 14)),\n",
       " ((datetime.datetime(2020, 1, 29, 19, 0), 166), (695.0099999999999, 45)),\n",
       " ((datetime.datetime(2020, 1, 16, 8, 0), 41), (736.1399999999996, 54)),\n",
       " ((datetime.datetime(2020, 1, 4, 20, 0), 129), (583.27, 38)),\n",
       " ((datetime.datetime(2020, 1, 2, 8, 0), 66), (197.69, 10))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(filter_outliers)\\\n",
    "    .map(prepare_for_grouping)\\\n",
    "    .reduceByKey(reduce)\\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc370edf",
   "metadata": {},
   "source": [
    "It might not be in the right format we want, we can unnest it to suit our use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6430c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap(row):\n",
    "    return RevenueRow(\n",
    "        hour= row[0][0],\n",
    "        zone = row[0][1],\n",
    "        revenue = row[1][0],\n",
    "        count = row[1][1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edcbbbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+------------------+---+\n",
      "|                 _1| _2|                _3| _4|\n",
      "+-------------------+---+------------------+---+\n",
      "|2020-01-15 20:00:00|260|163.90000000000003| 14|\n",
      "|2020-01-29 19:00:00|166| 695.0099999999999| 45|\n",
      "|2020-01-16 08:00:00| 41| 736.1399999999996| 54|\n",
      "|2020-01-04 20:00:00|129|            583.27| 38|\n",
      "|2020-01-02 08:00:00| 66|            197.69| 10|\n",
      "|2020-01-03 09:00:00| 61|            142.21|  9|\n",
      "|2020-01-17 21:00:00|236|              33.6|  4|\n",
      "|2020-01-12 12:00:00| 82|            290.41| 14|\n",
      "|2020-01-28 16:00:00|197| 831.4399999999998| 18|\n",
      "|2020-01-10 22:00:00| 95| 407.7100000000002| 37|\n",
      "|2020-01-10 01:00:00|215|            109.69|  2|\n",
      "|2020-01-07 18:00:00| 25| 554.2900000000001| 37|\n",
      "|2020-01-18 07:00:00| 55|              48.3|  1|\n",
      "|2020-01-28 09:00:00|166| 473.0200000000002| 36|\n",
      "|2020-01-12 15:00:00| 82| 265.7900000000001| 29|\n",
      "|2020-01-10 20:00:00| 66|            405.88| 21|\n",
      "|2020-01-31 15:00:00| 43|345.58000000000004| 19|\n",
      "|2020-01-31 21:00:00| 41| 588.1600000000001| 40|\n",
      "|2020-01-25 18:00:00| 65| 457.0600000000001| 28|\n",
      "|2020-01-26 14:00:00|166| 301.7900000000001| 26|\n",
      "+-------------------+---+------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd.filter(filter_outliers)\\\n",
    "    .map(prepare_for_grouping)\\\n",
    "    .reduceByKey(reduce)\\\n",
    "    .map(unwrap)\\\n",
    "    .toDF()\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cc60ba",
   "metadata": {},
   "source": [
    "You might notice that is not column name we can explicitly define it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "068d85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit the map function - unwrap\n",
    "from collections import namedtuple\n",
    "RevenueRow = namedtuple('RevenueRow',['hour','zone','revenue','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "132e675e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+-----+\n",
      "|               hour|zone|           revenue|count|\n",
      "+-------------------+----+------------------+-----+\n",
      "|2020-01-15 20:00:00| 260|163.90000000000003|   14|\n",
      "|2020-01-29 19:00:00| 166| 695.0099999999999|   45|\n",
      "|2020-01-16 08:00:00|  41| 736.1399999999996|   54|\n",
      "|2020-01-04 20:00:00| 129|            583.27|   38|\n",
      "|2020-01-02 08:00:00|  66|            197.69|   10|\n",
      "|2020-01-03 09:00:00|  61|            142.21|    9|\n",
      "|2020-01-17 21:00:00| 236|              33.6|    4|\n",
      "|2020-01-12 12:00:00|  82|            290.41|   14|\n",
      "|2020-01-28 16:00:00| 197| 831.4399999999998|   18|\n",
      "|2020-01-10 22:00:00|  95| 407.7100000000002|   37|\n",
      "|2020-01-10 01:00:00| 215|            109.69|    2|\n",
      "|2020-01-07 18:00:00|  25| 554.2900000000001|   37|\n",
      "|2020-01-18 07:00:00|  55|              48.3|    1|\n",
      "|2020-01-28 09:00:00| 166| 473.0200000000002|   36|\n",
      "|2020-01-12 15:00:00|  82| 265.7900000000001|   29|\n",
      "|2020-01-10 20:00:00|  66|            405.88|   21|\n",
      "|2020-01-31 15:00:00|  43|345.58000000000004|   19|\n",
      "|2020-01-31 21:00:00|  41| 588.1600000000001|   40|\n",
      "|2020-01-25 18:00:00|  65| 457.0600000000001|   28|\n",
      "|2020-01-26 14:00:00| 166| 301.7900000000001|   26|\n",
      "+-------------------+----+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd.filter(filter_outliers)\\\n",
    "    .map(prepare_for_grouping)\\\n",
    "    .reduceByKey(reduce)\\\n",
    "    .map(unwrap)\\\n",
    "    .toDF()\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a5ca1a",
   "metadata": {},
   "source": [
    "We can also define the schema we want \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4feba5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "result_schema = types.StructType([\n",
    "    types.StructField('hour', types.TimestampType(), True),\n",
    "    types.StructField('zone', types.IntegerType(), True),\n",
    "    types.StructField('revenue', types.DoubleType(), True),\n",
    "    types.StructField('count', types.IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9e8acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = rdd.filter(filter_outliers)\\\n",
    "    .map(prepare_for_grouping)\\\n",
    "    .reduceByKey(reduce)\\\n",
    "    .map(unwrap)\\\n",
    "    .toDF(result_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc03203f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hour', TimestampType(), True), StructField('zone', IntegerType(), True), StructField('revenue', DoubleType(), True), StructField('count', IntegerType(), True)])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58c4c198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save the file\n",
    "\n",
    "df_result.write.parquet('tmp/green-revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ddf1fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+-----+\n",
      "|               hour|zone|           revenue|count|\n",
      "+-------------------+----+------------------+-----+\n",
      "|2020-01-15 20:00:00| 260|163.90000000000003|   14|\n",
      "|2020-01-29 19:00:00| 166| 695.0099999999999|   45|\n",
      "|2020-01-16 08:00:00|  41| 736.1399999999996|   54|\n",
      "|2020-01-04 20:00:00| 129|            583.27|   38|\n",
      "|2020-01-02 08:00:00|  66|            197.69|   10|\n",
      "|2020-01-03 09:00:00|  61|            142.21|    9|\n",
      "|2020-01-17 21:00:00| 236|              33.6|    4|\n",
      "|2020-01-12 12:00:00|  82|            290.41|   14|\n",
      "|2020-01-28 16:00:00| 197| 831.4399999999998|   18|\n",
      "|2020-01-10 22:00:00|  95| 407.7100000000002|   37|\n",
      "|2020-01-10 01:00:00| 215|            109.69|    2|\n",
      "|2020-01-07 18:00:00|  25| 554.2900000000001|   37|\n",
      "|2020-01-18 07:00:00|  55|              48.3|    1|\n",
      "|2020-01-28 09:00:00| 166| 473.0200000000002|   36|\n",
      "|2020-01-12 15:00:00|  82| 265.7900000000001|   29|\n",
      "|2020-01-10 20:00:00|  66|            405.88|   21|\n",
      "|2020-01-31 15:00:00|  43|345.58000000000004|   19|\n",
      "|2020-01-31 21:00:00|  41| 588.1600000000001|   40|\n",
      "|2020-01-25 18:00:00|  65| 457.0600000000001|   28|\n",
      "|2020-01-26 14:00:00| 166| 301.7900000000001|   26|\n",
      "+-------------------+----+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f332dd1b",
   "metadata": {},
   "source": [
    "### mapPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78d05462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's say we have columns to predict 'trip_distance' with a well-trained model. We can slice data into chunk to\n",
    "# make prediction \n",
    "\n",
    "\n",
    "columns = ['VendorID', 'lpep_pickup_datetime', 'PULocationID', 'DOLocationID', 'trip_distance']\n",
    "\n",
    "duration_rdd = df_green \\\n",
    "    .select(columns) \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36a22bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of partition trick\n",
    "\n",
    "def apply_model_in_batch(partition):\n",
    "    return [1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6346e2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_rdd.mapPartitions(apply_model_in_batch).collect()\n",
    "\n",
    "# 4 partitions are returned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "128b374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check chunk size\n",
    "def apply_model_in_batch(partition):\n",
    "    cnt = 0\n",
    "    for row in partition:\n",
    "        cnt = cnt+1\n",
    "    return[cnt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5fda8e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1164928, 456935, 425873, 256781]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_rdd.mapPartitions(apply_model_in_batch).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e0517bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put everything into dataframe only count\n",
    "import pandas as pd\n",
    "\n",
    "def apply_model_in_batch(partition):\n",
    "    df = pd.DataFrame(partition,columns=columns)\n",
    "    cnt = len(df)\n",
    "    return[cnt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f146f325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1164928, 456935, 425873, 256781]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_rdd.mapPartitions(apply_model_in_batch).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4114db1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assuming we have a pretrained model as follow\n",
    "#model = ...\n",
    "\n",
    "def model_predict(df):\n",
    "#     y_pred = model.predict(df)\n",
    "    y_pred = df.trip_distance * 5\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "458834fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_in_batch(rows):\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    predictions = model_predict(df)\n",
    "    df['predicted_duration'] = predictions\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "25358772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_predicts = duration_rdd\\\n",
    "        .mapPartitions(apply_model_in_batch)\\\n",
    "        .toDF()\\\n",
    "        .drop('Index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "95dfa0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 39:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|predicted_duration|\n",
      "+------------------+\n",
      "|63.849999999999994|\n",
      "|              40.0|\n",
      "|              6.35|\n",
      "|              6.25|\n",
      "| 9.200000000000001|\n",
      "|               3.8|\n",
      "|16.599999999999998|\n",
      "|             11.05|\n",
      "|               4.5|\n",
      "|              30.5|\n",
      "|               8.7|\n",
      "|5.8999999999999995|\n",
      "|              11.0|\n",
      "|              15.2|\n",
      "|              4.25|\n",
      "|25.299999999999997|\n",
      "|7.8500000000000005|\n",
      "|              34.0|\n",
      "| 5.300000000000001|\n",
      "|              6.15|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_predicts.select('predicted_duration').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c0a45b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
