{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce4afbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "009ec3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/23 02:32:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Session\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"test\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6d874",
   "metadata": {},
   "source": [
    "## Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbe76013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read in data which have been saved\n",
    "\n",
    "df = spark.read.parquet(\"fhvhv/2021/01/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfaa26ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8b7c56",
   "metadata": {},
   "source": [
    "### Example of Transformation in Spark DataFrame\n",
    "\n",
    "Transformation is lazy and not directly executed. Example includes `.select`, `.filter`. If you're checking the Spark UI, no jobs will run for transformation in Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d547b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[pickup_datetime: timestamp, dropoff_datetime: timestamp]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"pickup_datetime\",\"dropoff_datetime\").filter(df.hvfhs_license_num == \"HV0003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb33547",
   "metadata": {},
   "source": [
    "### Example of Action in Spark DataFrame\n",
    "\n",
    "Action is eager and directly executed. Example included `.show()`, `.head()`. If you're checking the Spark UI, jobs will run for transformation in Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "771d5f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|    pickup_datetime|   dropoff_datetime|\n",
      "+-------------------+-------------------+\n",
      "|2021-01-05 22:14:07|2021-01-05 22:32:28|\n",
      "|2021-01-02 17:59:55|2021-01-02 18:10:39|\n",
      "|2021-01-02 23:57:54|2021-01-03 00:15:48|\n",
      "|2021-01-06 15:53:13|2021-01-06 16:07:07|\n",
      "|2021-01-07 07:35:24|2021-01-07 07:55:49|\n",
      "|2021-01-07 08:45:12|2021-01-07 08:51:17|\n",
      "|2021-01-02 15:44:26|2021-01-02 16:10:50|\n",
      "|2021-01-04 16:50:28|2021-01-04 16:57:43|\n",
      "|2021-01-03 10:30:34|2021-01-03 10:44:53|\n",
      "|2021-01-03 22:05:20|2021-01-03 22:27:55|\n",
      "|2021-01-04 08:01:02|2021-01-04 08:33:27|\n",
      "|2021-01-02 13:01:10|2021-01-02 13:08:11|\n",
      "|2021-01-06 17:12:27|2021-01-06 17:46:56|\n",
      "|2021-01-04 09:05:18|2021-01-04 09:27:50|\n",
      "|2021-01-06 16:46:47|2021-01-06 17:50:24|\n",
      "|2021-01-06 08:03:47|2021-01-06 08:17:43|\n",
      "|2021-01-04 06:45:42|2021-01-04 06:55:01|\n",
      "|2021-01-03 13:20:41|2021-01-03 13:31:11|\n",
      "|2021-01-03 17:30:33|2021-01-03 17:45:19|\n",
      "|2021-01-06 20:55:57|2021-01-06 21:02:01|\n",
      "+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"pickup_datetime\",\"dropoff_datetime\").filter(df.hvfhs_license_num == \"HV0003\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ecaf33",
   "metadata": {},
   "source": [
    "### Spark Built-in Functions\n",
    "In spark, the have pre-built in function that can be used for transformation. We use the built-in function called `to_date()` for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f86f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the functions\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "397b499b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+\n",
      "|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-----------+------------+------------+------------+\n",
      "| 2021-01-03|  2021-01-03|         255|          34|\n",
      "| 2021-01-05|  2021-01-05|         189|         107|\n",
      "| 2021-01-02|  2021-01-02|          88|         137|\n",
      "| 2021-01-02|  2021-01-03|         238|         224|\n",
      "| 2021-01-06|  2021-01-06|         169|         208|\n",
      "| 2021-01-07|  2021-01-07|          75|          88|\n",
      "| 2021-01-07|  2021-01-07|         210|         210|\n",
      "| 2021-01-02|  2021-01-02|         243|          69|\n",
      "| 2021-01-04|  2021-01-04|         250|         213|\n",
      "| 2021-01-03|  2021-01-03|          87|          79|\n",
      "| 2021-01-03|  2021-01-03|          68|         181|\n",
      "| 2021-01-04|  2021-01-04|          95|         236|\n",
      "| 2021-01-02|  2021-01-02|         262|         236|\n",
      "| 2021-01-04|  2021-01-04|         225|         233|\n",
      "| 2021-01-06|  2021-01-06|         237|          83|\n",
      "| 2021-01-05|  2021-01-05|         231|          87|\n",
      "| 2021-01-06|  2021-01-06|          22|          26|\n",
      "| 2021-01-04|  2021-01-04|         159|          75|\n",
      "| 2021-01-06|  2021-01-06|         109|         119|\n",
      "| 2021-01-06|  2021-01-06|         145|         229|\n",
      "+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding the date column to the spark dataframe\n",
    "df \\\n",
    "    .withColumn('pickup_date',F.to_date(df.pickup_datetime))\\\n",
    "    .withColumn('dropoff_date',F.to_date(df.dropoff_datetime))\\\n",
    "    .select('pickup_date','dropoff_date','PULocationID','DOLocationID')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd267a",
   "metadata": {},
   "source": [
    "### User-Defined Function\n",
    "\n",
    "In addition to the built-in function, PySpark provide the flexibility for developer to have their own function to have the data transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37d7395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_logic(base_num):\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}'\n",
    "    elif num % 6 == 0 :\n",
    "        return f'a/{num:03x}'\n",
    "    else:\n",
    "        return f'e/{num:03x}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49111a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a user define function\n",
    "\n",
    "business_logic_udf = F.udf(business_logic, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fafcb8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+------------+------------+\n",
      "|base_id|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "|  e/9ce| 2021-01-03|  2021-01-03|         255|          34|\n",
      "|  e/b42| 2021-01-05|  2021-01-05|         189|         107|\n",
      "|  e/b33| 2021-01-02|  2021-01-02|          88|         137|\n",
      "|  e/b38| 2021-01-02|  2021-01-03|         238|         224|\n",
      "|  e/b3b| 2021-01-06|  2021-01-06|         169|         208|\n",
      "|  e/b33| 2021-01-07|  2021-01-07|          75|          88|\n",
      "|  e/acc| 2021-01-07|  2021-01-07|         210|         210|\n",
      "|  e/acc| 2021-01-02|  2021-01-02|         243|          69|\n",
      "|  e/b35| 2021-01-04|  2021-01-04|         250|         213|\n",
      "|  s/b3d| 2021-01-03|  2021-01-03|          87|          79|\n",
      "|  e/a39| 2021-01-03|  2021-01-03|          68|         181|\n",
      "|  s/acd| 2021-01-04|  2021-01-04|          95|         236|\n",
      "|  s/b13| 2021-01-02|  2021-01-02|         262|         236|\n",
      "|  e/9ce| 2021-01-04|  2021-01-04|         225|         233|\n",
      "|  e/b14| 2021-01-06|  2021-01-06|         237|          83|\n",
      "|  e/9ce| 2021-01-05|  2021-01-05|         231|          87|\n",
      "|  e/9ce| 2021-01-06|  2021-01-06|          22|          26|\n",
      "|  a/a7a| 2021-01-04|  2021-01-04|         159|          75|\n",
      "|  e/b35| 2021-01-06|  2021-01-06|         109|         119|\n",
      "|  e/b43| 2021-01-06|  2021-01-06|         145|         229|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Applied the udf\n",
    "\n",
    "df \\\n",
    "    .withColumn('pickup_date',F.to_date(df.pickup_datetime))\\\n",
    "    .withColumn('dropoff_date',F.to_date(df.dropoff_datetime))\\\n",
    "    .withColumn('base_id',business_logic_udf(df.dispatching_base_num))\\\n",
    "    .select('base_id','pickup_date','dropoff_date','PULocationID','DOLocationID')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f0a4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
